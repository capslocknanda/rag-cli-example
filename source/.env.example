# LLM Engine Provider [ollama or gemini]
LLM_PROVIDER=ollama


# # If gemini is selected in provide make sure this key is correct
GEMINI_API_KEY=your_key
# If gemini is selected put gemini supported models
GEMINI_GENERATE_MODEL=gemini-2.5-flash-lite
GEMINI_EMBEDDING_MODEL=gemini-embedding-001


## Ollama Host
OLLAMA_HOST=http://localhost:11434/
# If ollama is selected put ollama pulled models
OLLAMA_GENERATE_MODEL=gemma3:27b-it-qat
#OLLAMA_GENERATE_MODEL=gemma3:1b-it-qat
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

## Qdrant Connection
# Use http://qdrant:6333 if running inside the same Docker network
# Use http://localhost:6333 if running your RAG script directly on your host
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your_secret_key_here  # Optional if you enabled security
QDRANT_COLLECTION_NAME=viewsonic_software_docs
QDRANT_SEARCH_LIMIT=4

## Vector Settings
# mxbai-embed-large dimension is 1024
VECTOR_DIMENSION=1024
VECTOR_DISTANCE_METRIC=Cosine

## API DETAILS
API_URL=http://localhost:8000/stream
